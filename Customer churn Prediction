# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve

# Machine learning models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
import xgboost as xgb

# Set random seed for reproducibility
np.random.seed(42)

# ----------------------------------
# 1. Load the Dataset
# ----------------------------------
# Adjust the file path as needed if running locally or on a notebook platform
data = pd.read_csv("Churn_Modelling.csv")
print("Dataset preview:")
print(data.head())

# ----------------------------------
# 2. Data Preprocessing
# ----------------------------------
# Drop columns that are not predictive
# Typically, 'RowNumber', 'CustomerId', and 'Surname' are not useful for prediction.
data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)

# Check the remaining columns
print("\nColumns after dropping non-predictive features:")
print(data.columns)

# The target column is 'Exited'
# Identify categorical features to encode: 'Geography' and 'Gender'
categorical_features = ['Geography', 'Gender']
for col in categorical_features:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])

print("\nDataset after encoding categorical features:")
print(data.head())

# ----------------------------------
# 3. Feature/Target Split and Scaling
# ----------------------------------
X = data.drop('Exited', axis=1)
y = data['Exited']

# Scale features (beneficial for models like Logistic Regression)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split (80% train, 20% test with stratification on the target)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2,
                                                    random_state=42, stratify=y)

# ----------------------------------
# 4. Hyperparameter Tuning with GridSearchCV
# ----------------------------------

# --- Logistic Regression ---
log_reg = LogisticRegression(max_iter=1000, random_state=42)
log_reg_params = {
    'C': [0.01, 0.1, 1, 10],
    'solver': ['lbfgs', 'liblinear']
}
grid_log_reg = GridSearchCV(log_reg, log_reg_params, cv=5, scoring='roc_auc', n_jobs=-1)
grid_log_reg.fit(X_train, y_train)
print("\nBest Logistic Regression Params:", grid_log_reg.best_params_)

# --- Random Forest ---
rf_clf = RandomForestClassifier(random_state=42)
rf_params = {
    'n_estimators': [100, 200],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5]
}
grid_rf = GridSearchCV(rf_clf, rf_params, cv=5, scoring='roc_auc', n_jobs=-1)
grid_rf.fit(X_train, y_train)
print("Best Random Forest Params:", grid_rf.best_params_)

# --- XGBoost ---
xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_params = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5],
    'learning_rate': [0.01, 0.1]
}
grid_xgb = GridSearchCV(xgb_clf, xgb_params, cv=5, scoring='roc_auc', n_jobs=-1)
grid_xgb.fit(X_train, y_train)
print("Best XGBoost Params:", grid_xgb.best_params_)

# ----------------------------------
# 5. Evaluation Function
# ----------------------------------
def evaluate_model(model, X_test, y_test, model_name="Model"):
    y_pred = model.predict(X_test)
    # Check if model supports probability predictions
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
    
    print(f"\n--- {model_name} Evaluation ---")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:")
    print(cm)
    
    # ROC AUC Score and ROC Curve
    if y_proba is not None:
        auc = roc_auc_score(y_test, y_proba)
        print("ROC AUC Score:", auc)
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        plt.figure(figsize=(6,4))
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')
        plt.plot([0,1], [0,1],'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve: {model_name}')
        plt.legend()
        plt.show()
    else:
        print("ROC AUC cannot be computed as the model does not support probability predictions.")

# ----------------------------------
# 6. Evaluate the Tuned Models
# ----------------------------------
best_log_reg = grid_log_reg.best_estimator_
best_rf = grid_rf.best_estimator_
best_xgb = grid_xgb.best_estimator_

evaluate_model(best_log_reg, X_test, y_test, model_name="Tuned Logistic Regression")
evaluate_model(best_rf, X_test, y_test, model_name="Tuned Random Forest")
evaluate_model(best_xgb, X_test, y_test, model_name="Tuned XGBoost")

# ----------------------------------
# 7. Stacking Ensemble (Meta Model)
# ----------------------------------
estimators = [
    ('lr', best_log_reg),
    ('rf', best_rf),
    ('xgb', best_xgb)
]

stacking_clf = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42),
    cv=5,
    n_jobs=-1
)
stacking_clf.fit(X_train, y_train)
evaluate_model(stacking_clf, X_test, y_test, model_name="Stacking Ensemble")

# ----------------------------------
# 8. Feature Importance for Tree-based Models
# ----------------------------------
def plot_feature_importance(model, feature_names, model_name):
    try:
        importance = model.feature_importances_
        feat_importance = pd.Series(importance, index=feature_names)
        feat_importance = feat_importance.sort_values(ascending=False)
        plt.figure(figsize=(8,4))
        feat_importance.plot(kind='bar')
        plt.title(f'Feature Importance: {model_name}')
        plt.ylabel('Importance Score')
        plt.show()
    except AttributeError:
        print(f"{model_name} does not have a feature_importances_ attribute.")

# Plot feature importance for Random Forest and XGBoost
plot_feature_importance(best_rf, X.columns, "Random Forest")
plot_feature_importance(best_xgb, X.columns, "XGBoost")
